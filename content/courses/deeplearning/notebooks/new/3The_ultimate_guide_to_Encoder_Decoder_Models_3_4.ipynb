{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The ultimate guide to Encoder Decoder Models 3/4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/3The_ultimate_guide_to_Encoder_Decoder_Models_3_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRzbFeCCZBw1"
      },
      "source": [
        "%%capture\n",
        "!pip install -qq git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myjTH2wFeWTU"
      },
      "source": [
        "# **Transformer-based Encoder-Decoder Models**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9YCIjCPInpe"
      },
      "source": [
        "The *transformer-based* encoder-decoder model was introduced by Vaswani et al. in the famous [Attention is all you need paper](https://arxiv.org/abs/1706.03762) and is today the *de-facto* standard encoder-decoder architecture in natural language processing (NLP).\n",
        "\n",
        "Recently, there has been a lot of research on different *pre-training* objectives for transformer-based encoder-decoder models, *e.g.* T5, Bart, Pegasus, ProphetNet, Marge, *etc*..., but the model architecture has stayed largely the same.\n",
        "\n",
        "The goal of the blog post is to give an **in-detail** explanation of **how** the transformer-based encoder-decoder architecture models *sequence-to-sequence* problems. We will focus on the mathematical model defined by the architecture and how the model can be used in inference. Along the way, we will give some background on sequence-to-sequence models in NLP and break down the *transformer-based* encoder-decoder architecture into its **encoder** and **decoder** part. We provide many illustrations and establish the link\n",
        "between the theory of *transformer-based* encoder-decoder models and their practical usage in ðŸ¤—Transformers for inference.\n",
        "Note that this blog post does *not* explain how such models can be trained - this will be the topic of a future blog post.\n",
        "\n",
        "Transformer-based encoder-decoder models are the result of years of research on *representation learning* and *model architectures*. \n",
        "This notebook provides a short summary of the history of neural encoder-decoder models. For more context, the reader is advised to read this awesome [blog post](https://ruder.io/a-review-of-the-recent-history-of-nlp/) by Sebastion Ruder. Additionally, a basic understanding of the *self-attention architecture* is recommended. \n",
        "The following blog post by Jay Alammar serves as a good refresher on the original Transformer model [here](http://jalammar.github.io/illustrated-transformer/).\n",
        "\n",
        "At the time of writing this notebook, ðŸ¤—Transformers comprises the encoder-decoder models *T5*, *Bart*, *MarianMT*, and *Pegasus*, which are summarized in the docs under [model summaries](https://huggingface.co/transformers/model_summary.html#sequence-to-sequence-models).\n",
        "\n",
        "The notebook is divided into four parts:\n",
        "\n",
        "- **Background** - *A short history of neural encoder-decoder models is given with a focus on on RNN-based models.* - [click here](https://colab.research.google.com/drive/18ZBlS4tSqSeTzZAVFxfpNDb_SrZfAOMf?usp=sharing)\n",
        "- **Encoder-Decoder** - *The transformer-based encoder-decoder model is presented and it is explained how the model is used for inference.* - [click here](https://colab.research.google.com/drive/1XpKHijllH11nAEdPcQvkpYHCVnQikm9G?usp=sharing)\n",
        "- **Encoder** - *The encoder part of the model is explained in detail.*\n",
        "- **Decoder** - *The decoder part of the model is explained in detail.* - to be published on *Thursday, 08.10.2020*\n",
        "\n",
        "Each part builds upon the previous part, but can also be read on its own. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jsJBw-nOIE_"
      },
      "source": [
        "## **Encoder**\n",
        "\n",
        "As mentioned in the previous section, the *transformer-based* encoder maps the input sequence to a contextualized encoding sequence:\n",
        "\n",
        "$$ f_{\\theta_{\\text{enc}}}: \\mathbf{X}_{1:n} \\to \\mathbf{\\overline{X}}_{1:n}. $$\n",
        "\n",
        "Taking a closer look at the architecture, the transformer-based encoder is a stack of residual *encoder blocks*.\n",
        "Each encoder block consists of a **bi-directional** self-attention layer, followed by two feed-forward layers. For simplicity, we disregard the normalization layers in this notebook. Also, we will not further discuss the role of the two feed-forward layers, but simply see it as a final vector-to-vector mapping required in each encoder block ${}^1$.\n",
        "The bi-directional self-attention layer puts each input vector $\\mathbf{x'}_j, \\forall j \\in \\{1, \\ldots, n\\}$ into relation with all input vectors $\\mathbf{x'}_1, \\ldots, \\mathbf{x'}_n$ and by doing so transforms the input vector $\\mathbf{x'}_j$ to a more \"refined\" contextual representation of itself, defined as $\\mathbf{x''}_j$.\n",
        "Thereby, the first encoder block transforms each input vector of the input sequence $\\mathbf{X}_{1:n}$ (shown in light green below) from a *context-independent* vector representation to a *context-dependent* vector representation, and the following encoder blocks further refine this contextual representation until the last encoder block outputs the final contextual encoding $\\mathbf{\\overline{X}}_{1:n}$ (shown in darker green below).\n",
        "\n",
        "Let's visualize how the encoder processes the input sequence \"I want to buy a car EOS\" to a contextualized encoding sequence. Similar to RNN-based encoders, transformer-based encoders also add a special \"end-of-sequence\" input vector to the input sequence to hint to the model that the input vector sequence is finished ${}^2$.\n",
        "\n",
        "![texte du lien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/Encoder_block.png)\n",
        "\n",
        "Our exemplary *transformer-based* encoder is composed of three encoder blocks, whereas the second encoder block is shown in more detail in the red box on the right for the first three input vectors $\\mathbf{x}_1, \\mathbf{x}_2 and \\mathbf{x}_3$.\n",
        "The bi-directional self-attention mechanism is illustrated by the fully-connected graph in the lower part of the red box and the two feed-forward layers are shown in the upper part of the red box. As stated before, we will focus only on the bi-directional self-attention mechanism.\n",
        "\n",
        "As can be seen each output vector of the self-attention layer $\\mathbf{x''}_i, \\forall i \\in \\{1, \\ldots, 7\\}$ depends *directly* on *all* input vectors $\\mathbf{x'}_1, \\ldots, \\mathbf{x'}_7$. This means, *e.g.* that the input vector representation of the word \"want\", *i.e.* $\\mathbf{x'}_2$, is put into direct relation with the word \"buy\", *i.e.* $\\mathbf{x'}_4$, but also with the word \"I\",*i.e.* $\\mathbf{x'}_1$. The output vector representation of \"want\", *i.e.* $\\mathbf{x''}_2$, thus represents a more refined contextual representation for the word \"want\".\n",
        "\n",
        "Let's take a deeper look at how bi-directional self-attention works.\n",
        "Each input vector $\\mathbf{x'}_i$ of an input sequence $\\mathbf{X'}_{1:n}$ of an encoder block is projected to a key vector $\\mathbf{k}_i$, value vector $\\mathbf{v}_i$ and query vector $\\mathbf{q}_i$ (shown in orange, blue, and purple respectively below) through three trainable weight matrices $\\mathbf{W}_q, \\mathbf{W}_v, \\mathbf{W}_k$:\n",
        "\n",
        "$$ \\mathbf{q}_i = \\mathbf{W}_q \\mathbf{x'}_i,$$\n",
        "$$ \\mathbf{v}_i = \\mathbf{W}_v \\mathbf{x'}_i,$$ \n",
        "$$ \\mathbf{k}_i = \\mathbf{W}_k \\mathbf{x'}_i, $$\n",
        "$$ \\forall i \\in \\{1, \\ldots n \\}.$$\n",
        "\n",
        "Note, that the **same** weight matrices are applied to each input vector $\\mathbf{x}_i, \\forall i \\in \\{i, \\ldots, n\\}$. After projecting each input vector $\\mathbf{x}_i$ to a query, key, and value vector, each query vector $\\mathbf{q}_j, \\forall j \\in \\{1, \\ldots, n\\}$ is compared to all key vectors $\\mathbf{k}_1, \\ldots, \\mathbf{k}_n$. The more similar one of the key vectors $\\mathbf{k}_1, \\ldots \\mathbf{k}_n$ is to a query vector $\\mathbf{q}_j$, the more important is the corresponding value vector $\\mathbf{v}_j$ for the output vector $\\mathbf{x''}_j$. More specifically, an output vector $\\mathbf{x''}_j$ is defined as the weighted sum of all value vectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_n$ plus the input vector $\\mathbf{x'}_j$. Thereby, the weights are proportional to the cosine similarity between $\\mathbf{q}_j$ and the respective key vectors $\\mathbf{k}_1, \\ldots, \\mathbf{k}_n$, which is mathematically expressed by $\\textbf{Softmax}(\\mathbf{K}_{1:n}^\\intercal \\mathbf{q}_j)$ as illustrated in the equation below.\n",
        "For a complete description of the self-attention layer, the reader is advised to take a look at [this](http://jalammar.github.io/illustrated-transformer/) blog post or the original [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "Alright, this sounds quite complicated. Let's illustrate the bi-directional self-attention layer for one of the query vectors of our example above. For simplicity, it is assumed that our exemplary *transformer-based* decoder uses only a single attention head `config.num_heads = 1` and that no normalization is applied.\n",
        "\n",
        "![texte du lien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/encoder_detail.png)\n",
        "\n",
        "On the left, the previously illustrated second encoder block is shown again and on the right, an in detail visualization of the bi-directional self-attention mechanism is given for the second input vector $\\mathbf{x'}_2$ that corresponds to the input word \"want\".\n",
        "At first all input vectors $\\mathbf{x'}_1, \\ldots, \\mathbf{x'}_7$ are projected to their respective query vectors $\\mathbf{q}_1, \\ldots, \\mathbf{q}_7$ (only the first three query vectors are shown in purple above), value vectors $\\mathbf{v}_1, \\ldots, \\mathbf{v}_7$ (shown in blue), and key vectors $\\mathbf{k}_1, \\ldots, \\mathbf{k}_7$ (shown in orange). The query vector $\\mathbf{q}_2$ is then multiplied by the transpose of all key vectors, *i.e.* $\\mathbf{K}_{1:7}^{\\intercal}$ followed by the softmax operation to yield the *self-attention weights*. The self-attention weights are finally multiplied by the respective value vectors and the input vector $\\mathbf{x'}_2$ is added to output the \"refined\" representation of the word \"want\", *i.e.* $\\mathbf{x''}_2$ (shown in dark green on the right).\n",
        "The whole equation is illustrated in the upper part of the box on the right.\n",
        "The multiplication of $\\mathbf{K}_{1:7}^{\\intercal}$ and $\\mathbf{q}_2$ thereby makes it possible to compare the vector representation of \"want\" to all other input vector representations \"I\", \"to\", \"buy\", \"a\", \"car\", \"EOS\" so that the self-attention weights mirror the importance each of the other input vector representations $\\mathbf{x'}_j \\text{, with } j \\ne 2$ for the refined representation $\\mathbf{x''}_2$ of the word \"want\".\n",
        "\n",
        "To further understand the implications of the bi-directional self-attention layer, let's assume the following sentence is processed: \"*The house is beautiful and well located in the middle of the city where it is easily accessible by public transport*\". The word \"it\" refers to \"house\", which is 12 \"positions away\". In transformer-based encoders, the bi-directional self-attention layer performs a single mathematical operation to put the input vector of \"house\" into relation with the input vector of \"it\" (compare to the first illustration of this section). In contrast, in an RNN-based encoder, a word that is 12 \"positions away\", would require at least 12 mathematical operations meaning that in an RNN-based encoder a linear number of mathematical operations are required. This makes it much harder for an RNN-based encoder to model long-range contextual representations.\n",
        "Also, it becomes clear that a transformer-based encoder is much less prone to lose important information than an RNN-based encoder-decoder model because the sequence length of the encoding is kept the same, *i.e.* $\\textbf{len}(\\mathbf{X}_{1:n}) = \\textbf{len}(\\mathbf{\\overline{X}}_{1:n}) = n$, while an RNN compresses the length from $\\textbf{len}((\\mathbf{X}_{1:n}) = n$ to just $\\textbf{len}(\\mathbf{c}) = 1$, which makes it very difficult for RNNs to effectively encode long-range dependencies between input words.\n",
        "\n",
        "In addition to making long-range dependencies more easily learnable, we can see that the Transformer architecture is able to process text in parallel.Mathematically, this can easily be shown by writing the self-attention formula as a product of query, key, and value matrices:\n",
        "\n",
        "$$\\mathbf{X''}_{1:n} = \\mathbf{V}_{1:n} \\text{Softmax}(\\mathbf{Q}_{1:n}^\\intercal \\mathbf{K}_{1:n}) + \\mathbf{X'}_{1:n}. $$\n",
        "\n",
        "The output $\\mathbf{X''}_{1:n} = \\mathbf{x''}_1, \\ldots, \\mathbf{x''}_n$ is computed via a series of matrix multiplications and a softmax operation, which can be parallelized effectively. \n",
        "Note, that in an RNN-based encoder model, the computation of the hidden state $\\mathbf{c}$ has to be done sequentially: Compute hidden state of the first input vector $\\mathbf{x}_1$, then compute the hidden state of the second input vector that depends on the hidden state of the first hidden vector, etc. The sequential nature of RNNs prevents effective parallelization and makes them much more inefficient compared to transformer-based encoder models on modern GPU hardware.\n",
        "\n",
        "Great, now we should have a better understanding of a) how transformer-based encoder models effectively model long-range contextual representations and b) how they efficiently process long sequences of input vectors. \n",
        "\n",
        "Now, let's code up a short example of the encoder part of our `MarianMT` encoder-decoder models to verify that the explained theory holds in practice.\n",
        "\n",
        "---\n",
        "${}^1$ An in-detail explanation of the role the feed-forward layers play in transformer-based models is out-of-scope for this notebook. It is argued in [Yun et. al, (2017)](https://arxiv.org/pdf/1912.10077.pdf) that feed-forward layers are crucial to map each contextual vector $\\mathbf{x'}_i$ individually to the desired output space, which the *self-attention* layer does not manage to do on its own. It should be noted here, that each output token $\\mathbf{x'}$ is processed by the same feed-forward layer. For more detail, the reader is advised to read the paper.\n",
        "\n",
        "${}^2$ However, the EOS input vector does not have to be appended to the input sequence, but has been shown to improve performance in many cases. In contrast to the *0th* $\\text{BOS}$ target vector of the transformer-based decoder is required as a starting input vector to predict a first target vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-21mDkdwHusE"
      },
      "source": [
        "%%capture\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUSpvzCs0398",
        "outputId": "11d76864-0ffa-498e-c5c0-124ad8051264",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "embeddings = model.get_input_embeddings()\n",
        "\n",
        "# create ids of encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# pass input_ids to encoder\n",
        "encoder_hidden_states = model.base_model.encoder(input_ids, return_dict=True).last_hidden_state\n",
        "\n",
        "# change the input slightly and pass to encoder\n",
        "input_ids_perturbed = tokenizer(\"I want to buy a house\", return_tensors=\"pt\").input_ids\n",
        "encoder_hidden_states_perturbed = model.base_model.encoder(input_ids_perturbed, return_dict=True).last_hidden_state\n",
        "\n",
        "# compare shape and encoding of first vector\n",
        "print(f\"Length of input embeddings {embeddings(input_ids).shape[1]}. Length of encoder_hidden_states {encoder_hidden_states.shape[1]}\")\n",
        "\n",
        "# compare values of word embedding of \"I\" for input_ids and perturbed input_ids\n",
        "print(\"Is encoding for `I` equal to its perturbed version?: \", torch.allclose(encoder_hidden_states[0, 0], encoder_hidden_states_perturbed[0, 0], atol=1e-3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of input embeddings 7. Length of encoder_hidden_states 7\n",
            "Is encoding for `I` equal to its perturbed version?:  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnRx-IamHnvN"
      },
      "source": [
        "We compare the length of the input word embeddings, *i.e.* `embeddings(input_ids)` corresponding to $\\mathbf{X}_{1:n}$, with the length of the `encoder_hidden_states`, corresponding to $\\mathbf{\\overline{X}}_{1:n}$.\n",
        "Also, we have forwarded the word sequence \"I want to buy a car\" and a slightly perturbated version \"I want to buy a house\" through the encoder to check if the first output encoding, corresponding to \"I\", differs when only the last word is changed in the input sequence.\n",
        "\n",
        "As expected the output length of the input word embeddings and encoder output encodings, *i.e.* $\\textbf{len}(\\mathbf{X}_{1:n})$ and $\\textbf{len}(\\mathbf{\\overline{X}}_{1:n})$, is equal.\n",
        "Second, it can be noted that the values of the encoded output vector of $\\mathbf{\\overline{x}}_1 = \\text{\"I\"}$ are different when the last word is changed from \"car\" to \"house\". This however should not come as a surprise if one has understood bi-directional self-attention.\n",
        "\n",
        "On a side-note, *autoencoding* models, such as BERT, have the exact same architecture as *transformer-based* encoder models. *Autoencoding* models leverage this architecture for massive self-supervised pre-training on open-domain text data so that they can map any word sequence to a deep bi-directional representation. In [Devlin et al. (2018)](https://arxiv.org/abs/1810.04805), the authors show that a pre-trained BERT model with a single task-specific classification layer on top can achieve SOTA results on eleven NLP tasks. All *autoencoding* models of ðŸ¤—Transformers can be found [here](https://huggingface.co/transformers/model_summary.html#autoencoding-models)."
      ]
    }
  ]
}