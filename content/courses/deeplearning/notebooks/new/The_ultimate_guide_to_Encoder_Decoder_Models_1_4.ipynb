{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The ultimate guide to Encoder Decoder Models 1/4",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/The_ultimate_guide_to_Encoder_Decoder_Models_1_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRzbFeCCZBw1"
      },
      "source": [
        "%%capture\n",
        "!pip install -qq git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myjTH2wFeWTU"
      },
      "source": [
        "# **The ultimate guide to Transformer-based Encoder-Decoder Models (1/4)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9YCIjCPInpe"
      },
      "source": [
        "The *transformer-based* encoder-decoder model was introduced by Vaswani et al. in the famous [Attention is all you need paper](https://arxiv.org/abs/1706.03762) and is today the *de-facto* standard encoder-decoder architecture in natural language processing (NLP).\n",
        "\n",
        "Recently, there has been a lot of research on different *pre-training* objectives for transformer-based encoder-decoder models, *e.g.* T5, Bart, Pegasus, ProphetNet, Marge, *etc*..., but the model architecture has stayed largely the same.\n",
        "\n",
        "The goal of the blog post is to give an **in-detail** explanation of **how** the transformer-based encoder-decoder architecture models *sequence-to-sequence* problems. We will focus on the mathematical model defined by the architecture and how the model can be used in inference. Along the way, we will give some background on sequence-to-sequence models in NLP and break down the *transformer-based* encoder-decoder architecture into its **encoder** and **decoder** part. We provide many illustrations and establish the link\n",
        "between the theory of *transformer-based* encoder-decoder models and their practical usage in ðŸ¤—Transformers for inference.\n",
        "Note that this blog post does *not* explain how such models can be trained - this will be the topic of a future blog post.\n",
        "\n",
        "Transformer-based encoder-decoder models are the result of years of research on *representation learning* and *model architectures*. \n",
        "This notebook provides a short summary of the history of neural encoder-decoder models. For more context, the reader is advised to read this awesome [blog post](https://ruder.io/a-review-of-the-recent-history-of-nlp/) by Sebastion Ruder. Additionally, a basic understanding of the *self-attention architecture* is recommended. \n",
        "The following blog post by Jay Alammar serves as a good refresher on the original Transformer model [here](http://jalammar.github.io/illustrated-transformer/).\n",
        "\n",
        "At the time of writing this notebook, ðŸ¤—Transformers comprises the encoder-decoder models *T5*, *Bart*, *MarianMT*, and *Pegasus*, which are summarized in the docs under [model summaries](https://huggingface.co/transformers/model_summary.html#sequence-to-sequence-models).\n",
        "\n",
        "The notebook is divided into four parts:\n",
        "\n",
        "- **Background** - *A short history of neural encoder-decoder models is given with a focus on on RNN-based models.*\n",
        "- **Encoder-Decoder** - *The transformer-based encoder-decoder model is presented and it is explained how the model is used for inference.* - to be published on *Tuesday, 06.10.2020*\n",
        "- **Encoder** - *The encoder part of the model is explained in detail.* - to be published on *Wednesday, 07.10.2020*\n",
        "- **Decoder** - *The decoder part of the model is explained in detail.* - to be published on *Thursday, 08.10.2020*\n",
        "\n",
        "Each part builds upon the previous part, but can also be read on its own. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVFKk5cyy_O0"
      },
      "source": [
        "## **Background** \n",
        "\n",
        "Tasks in natural language generation (NLG), a subfield of NLP, are best expressed as sequence-to-sequence problems. Such tasks can be defined as finding a model that maps a sequence of input words to a sequence of target words. Some classic examples are *summarization* and *translation*.\n",
        "In the following, we assume that each word is encoded into a vector representation. $n$ input words can then be represented as a sequence of $n$ input vectors:  \n",
        "\n",
        "$$\\mathbf{X}_{1:n} = \\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_n\\}.$$\n",
        "\n",
        "Consequently, sequence-to-sequence problems can be solved by finding a mapping $f$ from an input sequence of $n$ vectors $\\mathbf{X}_{1:n}$ to a sequence of $m$ target vectors $\\mathbf{Y}_{1:m}$, whereas the number of target vectors $m$ is unknown apriori and depends on the input sequence:\n",
        "\n",
        "$$ f: \\mathbf{X}_{1:n} \\to \\mathbf{Y}_{1:m}. $$\n",
        "\n",
        "[Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215) noted that deep neural networks (DNN)s, \"*despite their flexibility and power can only define a mapping whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality.*\"${}^1$\n",
        "\n",
        "Using a DNN model ${}^2$ to solve sequence-to-sequence problems would therefore mean that the number of target vectors $m$ has to be known *apriori* and would have to be independent of the input $\\mathbf{X}_{1:n}$.\n",
        "This is suboptimal because, for tasks in NLG, the number of target words usually depends on the input $\\mathbf{X}_{1:n}$ and not just on the input length $n$. \n",
        "*E.g.* An article of 1000 words can be summarized to both 200 words and 100 words depending on its content. \n",
        "\n",
        "In 2014, [Cho et al.](https://arxiv.org/pdf/1406.1078.pdf) and [Sutskever et al.](https://arxiv.org/abs/1409.3215) proposed to use an encoder-decoder model purely based on recurrent neural networks (RNNs) for *sequence-to-sequence* tasks. In contrast to DNNS, RNNs are capable of modeling a mapping to a variable number of target vectors. \n",
        "Let's dive a bit deeper into the functioning of RNN-based encoder-decoder models.\n",
        "\n",
        "During inference, the encoder RNN encodes an input sequence $\\mathbf{X}_{1:n}$ by successively updating its *hidden state*${}^3$. After having processed the last input vector $\\mathbf{x}_n$, the encoder's hidden state defines the input *encoding* $\\mathbf{c}$. Thus, the encoder defines the mapping:\n",
        "\n",
        "$$ f_{\\theta_{enc}}: \\mathbf{X}_{1:n} \\to \\mathbf{c}. $$\n",
        "\n",
        "Then, the decoder's hidden state is initialized with the input encoding and during inference, the decoder RNN is used to auto-regressively generate the target sequence.\n",
        "Let's explain. \n",
        "\n",
        "Mathematically, the decoder defines the probability distribution of a target sequence $\\mathbf{Y}_{1:m}$ given the hidden state $\\mathbf{c}$:\n",
        "\n",
        "$$ p_{\\theta_{dec}}(\\mathbf{Y}_{1:m} |\\mathbf{c}). $$\n",
        "\n",
        "By Bayes' rule the distribution can be decomposed into conditional distributions of single target vectors as follows:\n",
        "\n",
        "$$ p_{\\theta_{dec}}(\\mathbf{Y}_{1:m} |\\mathbf{c}) = \\prod_{i=1}^{m} p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{c}). $$\n",
        "\n",
        "Thus, if the architecture can model the conditional distribution of the next target vector, given all previous target vectors:\n",
        "\n",
        " $$ p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{c}), \\forall i \\in \\{1, \\ldots, m\\},$$ \n",
        "\n",
        " then it can model the distribution of any target vector sequence given the hidden state $\\mathbf{c}$ by simply multiplying all conditional probabilities. \n",
        "\n",
        "So how does the RNN-based decoder architecture model $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{c})$?\n",
        "\n",
        "In computational terms, the model sequentially maps the previous inner hidden state $\\mathbf{c}_{i-1}$ and the previous target vector $\\mathbf{y}_i$ to the current inner hidden state $\\mathbf{c}_i$ and a *logit vector* $\\mathbf{l}_i$ (shown in dark red below):\n",
        "\n",
        "$$ f_{\\theta_{\\text{dec}}}(\\mathbf{y}_{i-1}, \\mathbf{c}_{i-1}) \\to \\mathbf{l}_i, \\mathbf{c}_i.$$\n",
        "\n",
        "$\\mathbf{c}_0$ is thereby defined as $\\mathbf{c}$ being the output hidden state of the RNN-based encoder. \n",
        "Subsequently, the *softmax* operation is used to transform the logit vector $\\mathbf{l}_i$ to a conditional probablity distribution of the next target vector:\n",
        "\n",
        "$$ p(\\mathbf{y}_i | \\mathbf{l}_i) = \\textbf{Softmax}(\\mathbf{l}_i), \\text{ with } \\mathbf{l}_i = f_{\\theta_{\\text{dec}}}(\\mathbf{y}_{i-1}, \\mathbf{c}_{\\text{prev}}). $$\n",
        "\n",
        "For more detail on the logit vector and the resulting probability distribution, please see footnote ${}^4$.\n",
        "From the above equation, we can see that the distribution of the current target vector $\\mathbf{y}_i$ is directly conditioned on the previous target vector $\\mathbf{y}_{i-1}$ and the previous hidden state $\\mathbf{c}_{i-1}$. Because the previous hidden state $\\mathbf{c}_{i-1}$ depends on all previous target vectors $\\mathbf{y}_0, \\ldots, \\mathbf{y}_{i-2}$, it can be stated that the RNN-based decoder *implicitly* (*e.g.* *indirectly*) models the conditional distribution $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{c})$.\n",
        "\n",
        "The space of possible target vector sequences $\\mathbf{Y}_{1:m}$ is prohibitively large so that at inference, one has to rely on decoding methods ${}^5$ that efficiently sample high probability target vector sequences from $p_{\\theta_{dec}}(\\mathbf{Y}_{1:m} |\\mathbf{c})$.\n",
        "\n",
        "Given such a decoding method, during inference, the next input vector $\\mathbf{y}_i$ can then be sampled from $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{c})$ and is consequently appended to the input sequence so that the decoder RNN then models $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_{i+1} | \\mathbf{Y}_{0: i}, \\mathbf{c})$ to sample the next input vector $\\mathbf{y}_{i+1}$ and so on in *auto-regressive* fashion.\n",
        "\n",
        "An important feature of RNN-based encoder-decoder models is the definition of *special* vectors, such as the $\\text{EOS}$ and $\\text{BOS}$ vector. The $\\text{EOS}$ vector often represents the final input vector $\\mathbf{x}_n$ to \"cue\" the encoder that the input sequence has ended and also defines the end of the target sequence. As soon as the $\\text{EOS}$ is sampled from a logit vector, the generation is complete. \n",
        "The $\\text{BOS}$ vector represents the input vector $\\mathbf{y}_0$ fed to the decoder RNN at the very first decoding step. To output the first logit $\\mathbf{l}_1$, an input is required and since no input has been generated at the first step a special $\\text{BOS}$ input vector is fed to the decoder RNN.\n",
        "Ok - quite complicated! Let's illustrate and walk through an example.\n",
        "\n",
        "![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/rnn_seq2seq.png)\n",
        "\n",
        "The unfolded RNN encoder is colored in green and the unfolded RNN decoder is colored in red. \n",
        "\n",
        "The English sentence \"I want to buy a car\", represented by $\\mathbf{x}_1 = \\text{I}$, $\\mathbf{x}_2 = \\text{want}$, $\\mathbf{x}_3 = \\text{to}$, $\\mathbf{x}_4 = \\text{buy}$, $\\mathbf{x}_5 = \\text{a}$, $\\mathbf{x}_6 = \\text{car}$ and $\\mathbf{x}_7 = \\text{EOS}$ is translated into German: \"Ich will ein Auto kaufen\" defined as $\\mathbf{y}_0 = \\text{BOS}$, $\\mathbf{y}_1 = \\text{Ich}$, $\\mathbf{y}_2 = \\text{will}$, $\\mathbf{y}_3 = \\text{ein}$, $\\mathbf{y}_4 = \\text{Auto}, \\mathbf{y}_5 = \\text{kaufen}$ and $\\mathbf{y}_6=\\text{EOS}$.\n",
        "To begin with, the input vector $\\mathbf{x}_1 = \\text{I}$ is processed by the encoder RNN and updates its hidden state. Note that because we are only interested in the final encoder's hidden state $\\mathbf{c}$, we can disregard the RNN encoder's target vector.\n",
        "The encoder RNN then processes the rest of the input sentence $\\text{want}$, $\\text{to}$, $\\text{buy}$, $\\text{a}$, $\\text{car}$, $\\text{EOS}$ in the same fashion, updating its hidden state at each step until the vector $\\mathbf{x}_7={EOS}$ is reached ${}^6$. In the illustration above the horizontal arrow connecting the unfolded encoder RNN represents the sequential updates of the hidden state.\n",
        "The final hidden state of the encoder RNN, represented by $\\mathbf{c}$ then completely defines the *encoding* of the input sequence and is used as the initial hidden state of the decoder RNN. This can be seen as *conditioning* the decoder RNN on the encoded input. \n",
        "\n",
        "To generate the first target vector, the decoder is fed the $\\text{BOS}$ vector, illustrated as $\\mathbf{y}_0$ in the design above.\n",
        "The target vector of the RNN is then further mapped to the logit vector $\\mathbf{l}_1$ by means of the *LM Head* feed-forward layer to define the conditional distribution of the first target vector as explained above:\n",
        "\n",
        "$$ p_{\\theta_{dec}}(\\mathbf{y} | \\text{BOS}, \\mathbf{c}). $$\n",
        "\n",
        "The word $\\text{Ich}$ is sampled (shown by the grey arrow, connecting $\\mathbf{l}_1$ and $\\mathbf{y}_1$) and consequently the second target vector can be sampled:\n",
        "\n",
        "\n",
        "$$ \\text{will} \\sim p_{\\theta_{dec}}(\\mathbf{y} | \\text{BOS}, \\text{Ich}, \\mathbf{c}). $$\n",
        "\n",
        "\n",
        "And so on until at step $i=6$, the $\\text{EOS}$ vector is sampled from $\\mathbf{l}_6$ and the decoding is finished. The resulting target sequence amounts to $\\mathbf{Y}_{1:6} = \\{\\mathbf{y}_1, \\ldots, \\mathbf{y}_6\\}$, which is \"Ich will ein Auto kaufen\" in our example above. \n",
        "\n",
        "To sum it up, an RNN-based encoder-decoder model, represented by $f_{\\theta_{\\text{enc}}}$ and $ p_{\\theta_{\\text{dec}}}$ defines the distribution \n",
        "$p(\\mathbf{Y}_{1:m} | \\mathbf{X}_{1:n})$ by factorization:\n",
        "\n",
        "$$ p_{\\theta_{\\text{enc}}, \\theta_{\\text{dec}}}(\\mathbf{Y}_{1:m} | \\mathbf{X}_{1:n}) = \\prod_{i=1}^{m} p_{\\theta_{\\text{enc}}, \\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{X}_{1:n}) = \\prod_{i=1}^{m} p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{c}), \\text{ with } \\mathbf{c}=f_{\\theta_{enc}}(X). $$\n",
        "\n",
        "During inference, efficient decoding methods can auto-regressively generate the target sequence $\\mathbf{Y}_{1:m}$.\n",
        "\n",
        "The RNN-based encoder-decoder model took the NLG community by storm. In 2016, Google announced to fully replace its heavily feature engineered translation service by a single RNN-based encoder-decoder model (see [here](https://www.oreilly.com/radar/what-machine-learning-means-for-software-development/#:~:text=Machine%20learning%20is%20already%20making,of%20code%20in%20Google%20Translate.)). \n",
        "\n",
        "Nevertheless, RNN-based encoder-decoder models have two pitfalls. First, RNNs suffer from the vanishing gradient problem, making it very difficult to capture long-range dependencies, *cf.* [Hochreiter et al. (2001)](https://www.bioinf.jku.at/publications/older/ch7.pdf). Second, the inherent recurrent architecture of RNNs prevents efficient parallelization when encoding, *cf.* [Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762). \n",
        "\n",
        "---\n",
        "\n",
        "${}^1$ The original quote from the paper is \"*Despite their flexibility and power, DNNs can only be applied to problems whose inputs and targets can be sensibly encoded with vectors of fixed dimensionality*\", which is slightly adapted here.\n",
        "\n",
        "${}^2$ The same holds essentially true for convolutional neural networks (CNNs). While an input sequence of variable length can be fed into a CNN, the dimensionality of the target will always be dependent on the input dimensionality or fixed to a specific value.\n",
        "\n",
        "${}^3$ At the first step, the hidden state is initialized as a zero vector and fed to the RNN together with the first input vector $\\mathbf{x}_1$.\n",
        "\n",
        "${}^4$ A neural network can define a probability distribution over all words, *i.e.* $p(\\mathbf{y} | \\mathbf{c}, \\mathbf{Y}_{0: i-1})$ as follows. First, the network defines a mapping from the inputs $\\mathbf{c}, \\mathbf{Y}_{0: i-1}$ to an embedded vector representation $\\mathbf{y'}$, which corresponds to the RNN target vector. The embedded vector representation $\\mathbf{y'}$ is then passed to the \"language model head\" layer, which means that it is multiplied by the *word embedding matrix*, *i.e.* $\\mathbf{Y}^{\\text{vocab}}$, so that a score between $\\mathbf{y'}$ and each encoded vector $\\mathbf{y} \\in \\mathbf{Y}^{\\text{vocab}}$ is computed. The resulting vector is called the logit vector $\\mathbf{l} = \\mathbf{Y}^{\\text{vocab_size}} \\mathbf{y'}$ and can be mapped to a probability distribution over all words by applying a softmax operation: $p(\\mathbf{y} | \\mathbf{c}) = \\text{Softmax}(\\mathbf{Y}^{\\text{vocab_size}} \\mathbf{y'}) = \\text{Softmax}(\\mathbf{l})$.\n",
        "\n",
        "${}^5$ Beam-search decoding is an example of such a decoding method. Different decoding methods are out of scope for this notebook. The reader is advised to refer to this [interactive notebook](https://huggingface.co/blog/how-to-generate) on decoding methods.\n",
        "\n",
        "${}^6$ [Sutskever et al. (2014)](https://arxiv.org/abs/1409.3215) reverses the order of the input so that in the above example the input vectors would correspond to $\\mathbf{x}_1 = \\text{car}$, $\\mathbf{x}_2 = \\text{a}$, $\\mathbf{x}_3 = \\text{buy}$, $\\mathbf{x}_4 = \\text{to}$, $\\mathbf{x}_5 = \\text{want}$, $\\mathbf{x}_6 = \\text{I}$ and $\\mathbf{x}_7 = \\text{EOS}$. The motivation is to allow for a shorter connection between corresponding word pairs such as $\\mathbf{x}_6 = \\text{I}$ and $\\mathbf{y}_1 = \\text{Ich}$. The research group emphasizes that the reversal of the input sequence was a key reason for their model's improved performance on machine translation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhRmBmmOO8XL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}