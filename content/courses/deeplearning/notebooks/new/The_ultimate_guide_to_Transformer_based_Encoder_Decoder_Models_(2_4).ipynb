{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The ultimate guide to Transformer-based Encoder-Decoder Models (2/4)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/The_ultimate_guide_to_Transformer_based_Encoder_Decoder_Models_(2_4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRzbFeCCZBw1"
      },
      "source": [
        "%%capture\n",
        "!pip install -qq git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myjTH2wFeWTU"
      },
      "source": [
        "# **The ultimate guide to Transformer-based Encoder-Decoder Models (2/4)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9YCIjCPInpe"
      },
      "source": [
        "The *transformer-based* encoder-decoder model was introduced by Vaswani et al. in the famous [Attention is all you need paper](https://arxiv.org/abs/1706.03762) and is today the *de-facto* standard encoder-decoder architecture in natural language processing (NLP).\n",
        "\n",
        "Recently, there has been a lot of research on different *pre-training* objectives for transformer-based encoder-decoder models, *e.g.* T5, Bart, Pegasus, ProphetNet, Marge, *etc*..., but the model architecture has stayed largely the same.\n",
        "\n",
        "The goal of the blog post is to give an **in-detail** explanation of **how** the transformer-based encoder-decoder architecture models *sequence-to-sequence* problems. We will focus on the mathematical model defined by the architecture and how the model can be used in inference. Along the way, we will give some background on sequence-to-sequence models in NLP and break down the *transformer-based* encoder-decoder architecture into its **encoder** and **decoder** part. We provide many illustrations and establish the link\n",
        "between the theory of *transformer-based* encoder-decoder models and their practical usage in ðŸ¤—Transformers for inference.\n",
        "Note that this blog post does *not* explain how such models can be trained - this will be the topic of a future blog post.\n",
        "\n",
        "Transformer-based encoder-decoder models are the result of years of research on *representation learning* and *model architectures*. \n",
        "This notebook provides a short summary of the history of neural encoder-decoder models. For more context, the reader is advised to read this awesome [blog post](https://ruder.io/a-review-of-the-recent-history-of-nlp/) by Sebastion Ruder. Additionally, a basic understanding of the *self-attention architecture* is recommended. \n",
        "The following blog post by Jay Alammar serves as a good refresher on the original Transformer model [here](http://jalammar.github.io/illustrated-transformer/).\n",
        "\n",
        "At the time of writing this notebook, ðŸ¤—Transformers comprises the encoder-decoder models *T5*, *Bart*, *MarianMT*, and *Pegasus*, which are summarized in the docs under [model summaries](https://huggingface.co/transformers/model_summary.html#sequence-to-sequence-models).\n",
        "\n",
        "The notebook is divided into four parts:\n",
        "\n",
        "- **Background** - *A short history of neural encoder-decoder models is given with a focus on on RNN-based models.*- [click here](https://colab.research.google.com/drive/18ZBlS4tSqSeTzZAVFxfpNDb_SrZfAOMf?usp=sharing)\n",
        "- **Encoder-Decoder** - *The transformer-based encoder-decoder model is presented and it is explained how the model is used for inference.*\n",
        "- **Encoder** - *The encoder part of the model is explained in detail.* - to be published on *Wednesday, 07.10.2020*\n",
        "- **Decoder** - *The decoder part of the model is explained in detail.* - to be published on *Thursday, 08.10.2020*\n",
        "\n",
        "Each part builds upon the previous part, but can also be read on its own. \n",
        "\n",
        "Subscribe to our newsletter at [huggingface.curated.co](http://huggingface.curated.co) to get notified when the next full series is released ðŸ¤—."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILBApEhcMGy1"
      },
      "source": [
        "## **Encoder-Decoder** \n",
        "\n",
        "In 2017, Vaswani et al. introduced the **Transformer** and thereby gave birth to *transformer-based* encoder-decoder models. \n",
        "\n",
        "Analogous to RNN-based encoder-decoder models, transformer-based encoder-decoder models consist of an encoder and a decoder which are both stacks of *residual attention blocks*. \n",
        "The key innovation of transformer-based encoder-decoder models is that such residual attention blocks can process an input sequence $\\mathbf{X}_{1:n}$ of variable length $n$ without exhibiting a recurrent structure. Not relying on a recurrent structure allows transformer-based encoder-decoders to be highly parallelizable, which makes the model orders of magnitude more computationally efficient than RNN-based encoder-decoder models on modern hardware.\n",
        "\n",
        "As a reminder, to solve a *sequence-to-sequence* problem, we need to find a mapping of an input sequence $\\mathbf{X}_{1:n}$ to an output sequence $\\mathbf{Y}_{1:m}$ of variable length $m$. Let's see how transformer-based encoder-decoder models are used to find such a mapping.\n",
        "\n",
        "Similar to RNN-based encoder-decoder models, the transformer-based encoder-decoder models define a conditional distribution of target vectors $\\mathbf{Y}_{1:n}$ given an input sequence $\\mathbf{X}_{1:n}$:\n",
        "\n",
        "$$\n",
        "p_{\\theta_{\\text{enc}}, \\theta_{\\text{dec}}}(\\mathbf{Y}_{1:m} | \\mathbf{X}_{1:n}).\n",
        "$$\n",
        "\n",
        "The transformer-based encoder part encodes the input sequence $\\mathbf{X}_{1:n}$ to a *sequence* of *hidden states* $\\mathbf{\\overline{X}}_{1:n}$, thus defining the mapping: \n",
        "\n",
        "$$ f_{\\theta_{\\text{enc}}}: \\mathbf{X}_{1:n} \\to \\mathbf{\\overline{X}}_{1:n}. $$\n",
        "\n",
        "The transformer-based decoder part then models the conditional probability distribution of the target vector sequence $\\mathbf{Y}_{1:n}$ given the sequence of encoded hidden states $\\mathbf{\\overline{X}}_{1:n}$:\n",
        "\n",
        "$$ p_{\\theta_{dec}}(\\mathbf{Y}_{1:n} | \\mathbf{\\overline{X}}_{1:n}).$$\n",
        "\n",
        "By Bayes' rule, this distribution can be factorized to a product of conditional probability distribution of the target vector $\\mathbf{y}_i$ given the encoded hidden states $\\mathbf{\\overline{X}}_{1:n}$ and all previous target vectors $\\mathbf{Y}_{0:i-1}$:\n",
        "\n",
        "$$\n",
        "p_{\\theta_{dec}}(\\mathbf{Y}_{1:n} | \\mathbf{\\overline{X}}_{1:n}) = \\prod_{i=1}^{n} p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{\\overline{X}}_{1:n}). $$\n",
        "\n",
        "The transformer-based decoder hereby maps the sequence of encoded hidden states $\\mathbf{\\overline{X}}_{1:n}$ and all previous target vectors $\\mathbf{Y}_{0:i-1}$ to the *logit* vector $\\mathbf{l}_i$. The logit vector $\\mathbf{l}_i$ is then processed by the *softmax* operation to define the conditional distribution $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{\\overline{X}}_{1:n})$, just as it is done for RNN-based decoders. \n",
        "However, in contrast to RNN-based decoders, the distribution of the target vector $\\mathbf{y}_i$ is *explicitly* (or directly) conditioned on all previous target vectors $\\mathbf{y}_0, \\ldots, \\mathbf{y}_{i-1}$ as we will see later in more detail.\n",
        "The 0th target vector $\\mathbf{y}_0$ is hereby represented by a special \"begin-of-sentence\" $\\text{BOS}$ vector.\n",
        "\n",
        "Having defined the conditional distribution $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{\\overline{X}}_{1:n})$, we can now *auto-regressively* generate the output and thus define a mapping of an input sequence $\\mathbf{X}_{1:n}$ to an output sequence $\\mathbf{Y}_{1:m}$ at inference.\n",
        "\n",
        "Let's visualize the complete process of *auto-regressive* generation of *transformer-based* encoder-decoder models.\n",
        "\n",
        "![texte du lien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder.png)\n",
        "\n",
        "The transformer-based encoder is colored in green and the transformer-based decoder is colored in red. As in the previous section, we show how the English sentence \"I want to buy a car\", represented by $\\mathbf{x}_1 = \\text{I}$, $\\mathbf{x}_2 = \\text{want}$, $\\mathbf{x}_3 = \\text{to}$, $\\mathbf{x}_4 = \\text{buy}$, $\\mathbf{x}_5 = \\text{a}$, $\\mathbf{x}_6 = \\text{car}$, and $\\mathbf{x}_7 = \\text{EOS}$ is translated into German: \"Ich will ein Auto kaufen\" defined as $\\mathbf{y}_0 = \\text{BOS}$, $\\mathbf{y}_1 = \\text{Ich}$, $\\mathbf{y}_2 = \\text{will}$, $\\mathbf{y}_3 = \\text{ein}$, $\\mathbf{y}_4 = \\text{Auto}, \\mathbf{y}_5 = \\text{kaufen}$, and $\\mathbf{y}_6=\\text{EOS}$.\n",
        "\n",
        "To begin with, the encoder processes the complete input sequence $\\mathbf{X}_{1:7}$ = \"I want to buy a car\" (represented by the light green vectors) to a contextualized encoded sequence $\\mathbf{\\overline{X}}_{1:7}$. *E.g.* $\\mathbf{\\overline{x}}_4$ defines an encoding that depends not only on the input $\\mathbf{x}_4$ = \"buy\", but also on all other words \"I\", \"want\", \"to\", \"a\", \"car\" and \"EOS\", *i.e.* the context. \n",
        "\n",
        "Next, the input encoding $\\mathbf{\\overline{X}}_{1:7}$ together with the BOS vector, *i.e.* $\\mathbf{y}_0$, is fed to the decoder. The decoder processes the inputs $\\mathbf{\\overline{X}}_{1:7}$ and $\\mathbf{y}_0$ to the first logit $\\mathbf{l}_1$ (shown in darker red) to define the conditional distribution of the first target vector $\\mathbf{y}_1$:\n",
        "\n",
        "$$ p_{\\theta_{enc, dec}}(\\mathbf{y} | \\mathbf{y}_0, \\mathbf{X}_{1:7}) = p_{\\theta_{enc, dec}}(\\mathbf{y} | \\text{BOS}, \\text{I want to buy a car EOS}) = p_{\\theta_{dec}}(\\mathbf{y} | \\text{BOS}, \\mathbf{\\overline{X}}_{1:7}). $$\n",
        "\n",
        "Next, the first target vector $\\mathbf{y}_1$ = $\\text{Ich}$ is sampled from the distribution (represented by the grey arrows) and can now be fed to the decoder again. The decoder now processes both $\\mathbf{y}_0$  = \"BOS\" and $\\mathbf{y}_1$ = \"Ich\" to define the conditional distribution of the second target vector $\\mathbf{y}_2$:\n",
        "\n",
        "$$ p_{\\theta_{dec}}(\\mathbf{y} | \\text{BOS Ich}, \\mathbf{\\overline{X}}_{1:7}). $$\n",
        "\n",
        "We can sample again and produce the target vector $\\mathbf{y}_2$ = \"will\". We continue in auto-regressive fashion until at step 6 the EOS vector is sampled from the conditional distribution: \n",
        "\n",
        "$$ \\text{EOS} \\sim p_{\\theta_{dec}}(\\mathbf{y} | \\text{BOS Ich will ein Auto kaufen}, \\mathbf{\\overline{X}}_{1:7}). $$\n",
        "\n",
        "And so on in auto-regressive fashion.\n",
        "\n",
        "It is important to understand that the encoder is only used in the first forward pass to map $\\mathbf{X}_{1:n}$ to $\\mathbf{\\overline{X}}_{1:n}$.\n",
        "As of the second forward pass, the decoder can directly make use of the previously calculated encoding $\\mathbf{\\overline{X}}_{1:n}$. \n",
        "For clarity, let's illustrate the first and the second forward pass for our example above.\n",
        "\n",
        "![texte du lien](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/encoder_decoder/EncoderDecoder_step_by_step.png)\n",
        "\n",
        "As can be seen, only in step $i=1$ do we have to encode \"I want to buy a car EOS\" to $\\mathbf{\\overline{X}}_{1:7}$. At step $i=2$, the contextualized encodings of \"I want to buy a car EOS\" are simply reused by the decoder.\n",
        "\n",
        "In ðŸ¤—Transformers, this auto-regressive generation is done under-the-hood when calling the `.generate()` method. Let's use one of our translation models to see this in action.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMTXlLtB0Wc-",
        "outputId": "e8043523-feec-4a65-c98a-60b8de92852a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "# create ids of encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# translate example\n",
        "output_ids = model.generate(input_ids)[0]\n",
        "\n",
        "# decode and print\n",
        "print(tokenizer.decode(output_ids))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<pad> Ich will ein Auto kaufen\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO6KM8GQ1Bbd"
      },
      "source": [
        "Calling `.generate()` does many things under-the-hood.  First, it passes the `input_ids` to the encoder. Second, it passes a pre-defined <BOS> token, which is the $\\text{<pad>}$ symbol in the case of `MarianMTModel` along with the encoded `input_ids` to the decoder. Third, it applies the beam search decoding mechanism to auto-regressively sample the next output word of the *last* decoder output ${}^1$. For more detail on how beam search decoding works, one is advised to read [this](https://huggingface.co/blog/how-to-generate) blog post.\n",
        "\n",
        "In the Appendix, we have included a code snippet that shows how a simple generation method can be implemented \"from scratch\". To fully understand how *auto-regressive* generation works under-the-hood, it is highly recommended to read the Appendix.\n",
        "\n",
        "To sum it up:\n",
        "- The transformer-based encoder defines a mapping from the input sequence $\\mathbf{X}_{1:n}$ to a contextualized encoding sequence $\\mathbf{\\overline{X}}_{1:n}$.\n",
        "- The transformer-based decoder defines the conditional distribution \n",
        "$p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{\\overline{X}}_{1:n})$. \n",
        "- Given an appropriate decoding mechanism, the output sequence $\\mathbf{Y}_{1:m}$ can auto-regressively be sampled from $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{\\overline{X}}_{1:n}), \\forall i \\in \\{1, \\ldots, m\\}$. \n",
        "\n",
        "Great, now that we have gotten a general overview of how *transformer-based* encoder-decoder models work, we can dive deeper into both the encoder and decoder part of the model. More specifically, we will see exactly how the encoder makes use of the self-attention layer to yield a sequence of context-dependent vector encodings and how self-attention layers allow for efficient parallelization.\n",
        "Then, we will explain in detail how the self-attention layer works in the decoder model and how the decoder is conditioned on the encoder's output with *cross-attention* layers to define the conditional distribution $p_{\\theta_{\\text{dec}}}(\\mathbf{y}_i | \\mathbf{Y}_{0: i-1}, \\mathbf{\\overline{X}}_{1:n})$.\n",
        "Along, the way it will become obvious how transformer-based encoder-decoder models solve the long-range dependencies problem of RNN-based encoder-decoder models.\n",
        "\n",
        "---\n",
        "${}^1$ In the case of `\"Helsinki-NLP/opus-mt-en-de\"`, the decoding parameters can be accessed [here](https://s3.amazonaws.com/models.huggingface.co/bert/Helsinki-NLP/opus-mt-en-de/config.json), where we can see that model applies beam search with `num_beams=6`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF0rjj-TmjYa"
      },
      "source": [
        "## **Appendix**\n",
        "\n",
        "As mentioned above, the following code snippet shows how one can program \n",
        "a simple generation method for *transformer-based* encoder-decoder models.\n",
        "Here, we implement a simple *greedy* decoding method using `torch.argmax` to sample the target vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSSQdRoOZG-w",
        "outputId": "2c4ed494-d94f-4f7a-d530-d587e4260905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "# create ids of encoded input vectors\n",
        "input_ids = tokenizer(\"I want to buy a car\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "# create BOS token\n",
        "decoder_input_ids = tokenizer(\"<pad>\", add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
        "\n",
        "assert decoder_input_ids[0, 0].item() == model.config.decoder_start_token_id, \"`decoder_input_ids` should correspond to `model.config.decoder_start_token_id`\"\n",
        "\n",
        "# STEP 1\n",
        "\n",
        "# pass input_ids to encoder and to decoder and pass BOS token to decoder to retrieve first logit\n",
        "outputs = model(input_ids, decoder_input_ids=decoder_input_ids, return_dict=True)\n",
        "\n",
        "# get encoded sequence\n",
        "encoded_sequence = (outputs.encoder_last_hidden_state,)\n",
        "# get logits\n",
        "lm_logits = outputs.logits\n",
        "\n",
        "# sample last token with highest prob\n",
        "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
        "\n",
        "# concat\n",
        "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
        "\n",
        "# STEP 2\n",
        "\n",
        "# reuse encoded_inputs and pass BOS + \"Ich\" to decoder to second logit\n",
        "lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
        "\n",
        "# sample last token with highest prob again\n",
        "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
        "\n",
        "# concat again\n",
        "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
        "\n",
        "# STEP 3\n",
        "lm_logits = model(None, encoder_outputs=encoded_sequence, decoder_input_ids=decoder_input_ids, return_dict=True).logits\n",
        "next_decoder_input_ids = torch.argmax(lm_logits[:, -1:], axis=-1)\n",
        "decoder_input_ids = torch.cat([decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
        "\n",
        "# let's see what we have generated so far!\n",
        "print(f\"Generated so far: {tokenizer.decode(decoder_input_ids[0], skip_special_tokens=True)}\")\n",
        "\n",
        "# This can be written in a loop as well.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated so far: Ich Ich\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfU7AqfHcDBS"
      },
      "source": [
        "In this code example, we show exactly what was described earlier. We pass an input \"I want to buy a car\" together with the $\\text{BOS}$ token to the encoder-decoder model and sample from the first logit $\\mathbf{l}_1$ (*i.e.* the first `lm_logits` line). Hereby, our sampling strategy is simple to greedily choose the next decoder input vector that has the highest probability. In an auto-regressive fashion, we then pass the sampled decoder input vector together with the previous inputs to the encoder-decoder model and sample again. We repeat this a third time. As a result, the model has generated the words \"Ich Ich\". The first word is spot-on! The second word is not that great. \n",
        "We can see here, that a good decoding method is key for a successful sequence generation from a given model distribution.\n",
        "\n",
        " In practice, more complicated decoding methods are used to sample the `lm_logits`. Most of which are covered in [this](https://huggingface.co/blog/how-to-generate) blog post."
      ]
    }
  ]
}