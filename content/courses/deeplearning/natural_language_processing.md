---
title: Natural Language Procession
linktitle: Natural Language Procession
toc: true
type: docs
date: "2019-05-05T00:00:00+01:00"
draft: false
menu:
  example:
    parent: Main Course
    weight: 70

# Prev/next pager order (if `docs_section_pager` enabled in `params.toml`)
weight: 70
---

{{< figure library="true" src="nlp2.png" >}}

### Lecture Video (Portuguese)

* [Natural Language Processing](https://drive.google.com/file/d/1wOQPhzwYCq7uZYR4rn_nt4I7RnB-Fw2M/view?usp=sharing)

### Slides (English)

* [Natural Language Processing](https://drive.google.com/file/d/1kVLuiU6mkWYF2lyAHMtZeG0rSX1wclNu/view?usp=sharing)

### Code (Google Colab, Notebooks and More)

* [TensorFlow: Word Embeddings](https://githubtocolab.com/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/tensorflow/word_embeddings.ipynb)

* [TensorFlow: Transformer Model for Language Understanding](https://githubtocolab.com/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/tensorflow/transformer.ipynb)

* [TensorFlow: Fine-tuning a BERT Model](https://githubtocolab.com/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/tensorflow/fine_tuning_bert.ipynb)

* [Fine-tuning with HuggingFace Datasets library and PyTorch](https://githubtocolab.com/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/pytorch/example_fine_tuning_hf_datasets.ipynb)

* [Sentence Embeddings using Siamese BERT-Networks](https://githubtocolab.com/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/SiameseBERT_SemanticSearch.ipynb)

* [Keras: Natural Language Processing](https://keras.io/examples/nlp/)

* [Huggingface: Quick tour](https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/quicktour.ipynb)

* [Huggingface: Preprocessing data](https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/preprocessing.ipynb)

* [Huggingface: Fine-tuning a pretrained model](https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/training.ipynb)

* [Huggingface: Fine-tuning with custom datasets](https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/custom_datasets.ipynb)

* [Huggingface: Summary of the tokenizers](https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/tokenizer_summary.ipynb)

* [Huggingface: Multi-lingual models](https://colab.research.google.com/github/huggingface/notebooks/blob/master/transformers_doc/multilingual.ipynb)

* [Huggingface: Transformers Notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)

### Additional Resources

* [The Super Duper NLP Repo](https://notebooks.quantumstat.com/)

* [Transformers Notebooks](https://github.com/huggingface/transformers/tree/master/notebooks)

* [Huggingface Transformers](https://github.com/huggingface/transformers)

* [Simple Transformers](https://github.com/ThilinaRajapakse/simpletransformers)

* [Sentence Transformers](https://github.com/UKPLab/sentence-transformers)

* [SimpleTransformers: Transformers Made Easy](https://wandb.ai/wandb/gallery/reports/SimpleTransformers-Transformers-Made-Easy--VmlldzoyNDQzNTg)
